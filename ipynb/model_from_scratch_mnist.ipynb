{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780a5bd3",
   "metadata": {},
   "source": [
    "# Creating a Digit Classifier (Almost) from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9548ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krm/mambaforge/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_cuda.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1973927",
   "metadata": {},
   "source": [
    "We're creating a model that can classify any images as a 3 or a 7. We'll use a sample of MNIST that contains just these.\n",
    "\n",
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e7d01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78f1f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/krm/.fastai/data/mnist_sample/labels.csv'),Path('/home/krm/.fastai/data/mnist_sample/train'),Path('/home/krm/.fastai/data/mnist_sample/valid')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccddb63",
   "metadata": {},
   "source": [
    "The sample data is divided into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9525ec2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#2) [Path('/home/krm/.fastai/data/mnist_sample/train/7'),Path('/home/krm/.fastai/data/mnist_sample/train/3')],\n",
       " (#2) [Path('/home/krm/.fastai/data/mnist_sample/valid/7'),Path('/home/krm/.fastai/data/mnist_sample/valid/3')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/\"train\").ls(), (path/\"valid\").ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e10436",
   "metadata": {},
   "source": [
    "Get a list of the training set of 3s and 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee0f5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = (path/\"train\"/\"3\").ls().sorted()\n",
    "sevens = (path/\"train\"/\"7\").ls().sorted()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac100836",
   "metadata": {},
   "source": [
    "Have a look at one of the 3s. We use the `Image` class from PIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d9c6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nGNgGLyAUSh2yb////79m44hxasy/e/fv3///r31/ioXumTk379/v64JCgpimvJ3GlSMBSH9f333KQYGBlVZBjV0nYLTjRkYGBgYNJ7+/TsZu6OCz739+/cyBxYZt/7Pv//+/bvVCIscyxmIc7PgIkwISUZ5BoYHqxL/CmO1MG+9jxADw88vdtjdw8DAwHD1bw0WY6FgLUMQbp3v/p5D08m897woVISb4TCacvEbf+dD1AX8/huBbtjyv38VGRgYGOy//b0ijy4psvfv7QIGhvCLyIEABzZf/v44efLj37+XPbC4MvDn379///69iD2AhE/+/XszGiMVUB8AAIBLZ4nJiClnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img3_path = threes[2]\n",
    "img3 = Image.open(img3_path)\n",
    "img3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8c273d",
   "metadata": {},
   "source": [
    "We can 'see' the image as a collection of digits using numpy arrays of pytorch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d85d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,  13,  36],\n",
       "       [  0,   0,   0,   0,  89, 253],\n",
       "       [  0,   0,   0,   0,  89, 253],\n",
       "       [  0,   0,   0,   0,  17, 151]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(img3)[4:10, 4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8730aae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  13,  36],\n",
       "        [  0,   0,   0,   0,  89, 253],\n",
       "        [  0,   0,   0,   0,  89, 253],\n",
       "        [  0,   0,   0,   0,  17, 151]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(img3)[4:10, 4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8282421",
   "metadata": {},
   "source": [
    "Next we create tensors for each of the 3s and 7s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c1eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_tensors = [tensor(Image.open(path)) for path in threes]\n",
    "seven_tensors = [tensor(Image.open(path)) for path in sevens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8ebff9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(three_tensors), len(seven_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9cee3",
   "metadata": {},
   "source": [
    "Use fastai's show_image to see a seven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ccc525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(seven_tensors[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1cd89",
   "metadata": {},
   "source": [
    "We stack each list of tensors into a single one of 3 axes (rank 3), convert to float for some operations. We also scale to values between 0 and 1 (better for the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e54f65b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_threes = torch.stack(three_tensors).float() / 255\n",
    "stacked_sevens = torch.stack(seven_tensors).float() / 255\n",
    "stacked_threes.shape, stacked_sevens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7dfac",
   "metadata": {},
   "source": [
    "We create a training input collection by concatenating the two stacked tensors into one. We use the `view` method to reshape the tensor into two dimensions. `-1` means making the first dimension as big as possible to accomodate the new shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "439bd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36dcd0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d635368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2efe0d",
   "metadata": {},
   "source": [
    "Each item along axis 0 is now a list of 784 floats representing a single image.\n",
    "\n",
    "We next create labels for our training input. Our objective is to classify whether an image is a 3 or not. The labels for 3s will be 1 (True) and for 7s will be 0 (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df28c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_flat = tensor([1] * len(threes) + [0] * len(sevens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa67743a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad92665",
   "metadata": {},
   "source": [
    "We need to have a 2D tensor with the second dimension being a size of 1. We use pytorch's `unsqueeze` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f60d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y_flat.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d37ddca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dadefa",
   "metadata": {},
   "source": [
    "In pytorch, a dataset needs to return a tuple of (x, y) when indexed. We therefore zip training input and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "290986ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(zip(train_x, train_y))\n",
    "x0, y0 = dataset[0]\n",
    "x0.shape, y0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad28448",
   "metadata": {},
   "source": [
    "We do the same preparation to the validation data as we've done for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8f59e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_threes = (path/\"valid\"/\"3\").ls().sorted()\n",
    "v_sevens = (path/\"valid\"/\"7\").ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "476331ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_three_tensors = [tensor(Image.open(path)) for path in v_threes]\n",
    "v_seven_tensors = [tensor(Image.open(path)) for path in v_sevens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccded4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_v_threes = torch.stack(v_three_tensors).float()/255\n",
    "stacked_v_sevens = torch.stack(v_seven_tensors).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e337bb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038, 784])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x = torch.cat([stacked_v_threes, stacked_v_sevens]).view(-1, 28 * 28)\n",
    "valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d606dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y = tensor([1] * len(v_threes) + [0] * len(v_sevens)).unsqueeze(1)\n",
    "valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0cc08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = list(zip(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04d2ba4",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The initial model will be a linear function with weights for each pixel and a bias. We'll need to calculate gradients for each of these, so we call `requires_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f644e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size) * std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36818736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), torch.Size([1]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = init_params((28*28, 1))\n",
    "bias = init_params(1)\n",
    "weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe5d6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x_batch): return x_batch@weights + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56afb247",
   "metadata": {},
   "source": [
    "Next, we need to define a loss function that is sensitive to small changes in the parameters. For each batch of predictions, we calculate distance from the target values, and get the mean.\n",
    "\n",
    "We also need to coerce the predictions to values between 0 and 1, so we'll use a sigmoid function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0bca106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c9c1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    predictions = sigmoid(predictions)\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e61428c",
   "metadata": {},
   "source": [
    "We now have enough to calculate gradients. We define a procedure that makes predictions, calculates the loss, then calculates the gradients that would minimise the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae62c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(x_batch, y_batch, model):\n",
    "    predictions = model(x_batch)\n",
    "    loss = mnist_loss(predictions, y_batch)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7552c4",
   "metadata": {},
   "source": [
    "We need to iteratively process mini batches until we exhaust the entire dataset. The `fastai` library provides a `DataLoader` class that will shuffle the dataset and provide batches of input and their corresponding targets according to the batch size we provide. We can iterate through this to process the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ad9fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(dataset, batch_size=256)\n",
    "valid_dl = DataLoader(validation_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1756ca",
   "metadata": {},
   "source": [
    "Now we can encode the process of training through an entire epoch. With each mini-batch, we optimize the parameters using the gradients and learning rate. We're adjusting each parameter in opposite direction of the gradient to get closer to a minimized loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "914155aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, learning_rate, params):\n",
    "    for x, y in train_dl:\n",
    "        calculate_gradients(x, y, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad * learning_rate\n",
    "            p.grad.zero_()  # reset gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54225bc7",
   "metadata": {},
   "source": [
    "We'll also calculate an accuracy metric for each epoch so that we can observe that the accuracy is improving with each successive epoch.\n",
    "\n",
    "We first calculate the metric for each batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df56a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(predictions, targets):\n",
    "    predictions = sigmoid(predictions)\n",
    "    correct = (predictions > 0.5) == targets\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba43060",
   "metadata": {},
   "source": [
    "Then average it for the entire epoch (rounded off to 4 decimal places):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6efd9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    accuracies = [batch_accuracy(model(x_batch), y_batch) for x_batch, y_batch in valid_dl]\n",
    "    return round(torch.stack(accuracies).mean().item(), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7456dfd7",
   "metadata": {},
   "source": [
    "We can now see the model performance for the entire epoch. We define a procedure that takes in the model, parameters, learning rate and number of epochs and prints out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21c04573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model, params, learning_rate, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model, learning_rate, params)\n",
    "        print(validate_epoch(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67cd044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.662\n",
      "0.8637\n",
      "0.9233\n",
      "0.9399\n",
      "0.9487\n",
      "0.9546\n",
      "0.9604\n",
      "0.9638\n",
      "0.9658\n",
      "0.9673\n",
      "0.9678\n",
      "0.9682\n",
      "0.9697\n",
      "0.9707\n",
      "0.9721\n",
      "0.9736\n",
      "0.9736\n",
      "0.9736\n",
      "0.9741\n",
      "0.9741\n"
     ]
    }
   ],
   "source": [
    "learn(linear, params=(weights, bias), learning_rate=1.0, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c1dd0",
   "metadata": {},
   "source": [
    "We see that the accuracy gradually improves to approx 96%. \n",
    "\n",
    "\n",
    "Pytorch provides some handy functionality that we can use to simplify the process above.\n",
    "`nn.Linear` will combine what `init_params` and `linear` do together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03999b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = nn.Linear(28 * 28, 1)\n",
    "weights, bias = linear_model.parameters()\n",
    "weights.shape, bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9077d81",
   "metadata": {},
   "source": [
    "We can also create an optimizer class that will optimize parameters using an interface that can plug into pytorch functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bef8c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptimizer:\n",
    "    def __init__(self, params, learning_rate):\n",
    "        self.params = params\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.data -= p.grad.data * self.learning_rate\n",
    "    \n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params:\n",
    "            p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59f64738",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BasicOptimizer(linear_model.parameters(), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772a2e9",
   "metadata": {},
   "source": [
    "We can re-write `train_epoch` and `learn` to use the new optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6879b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model):\n",
    "    for x, y in train_dl:\n",
    "        calculate_gradients(x, y, model)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "def learn(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f4c77",
   "metadata": {},
   "source": [
    "We should get similar results to the previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d5e4e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n",
      "0.4932\n"
     ]
    }
   ],
   "source": [
    "learn(linear_model, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a64b9159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 3.4429e-02,  1.7063e-03, -3.5834e-03,  3.0296e-03, -8.0531e-03,\n",
       "         -1.7961e-02, -3.0332e-02, -1.6867e-02,  3.5372e-02,  1.9368e-02,\n",
       "         -1.6617e-02, -1.5027e-02, -1.1246e-02,  9.8497e-03,  1.4193e-02,\n",
       "          1.6653e-02, -3.4344e-02,  2.2654e-02,  1.2755e-02,  3.3253e-03,\n",
       "          3.4526e-02,  2.2616e-02,  3.2427e-02, -1.9899e-02, -1.1922e-02,\n",
       "          3.2503e-02, -1.0366e-02,  3.1291e-02,  8.3053e-03,  1.8344e-02,\n",
       "          1.7709e-02,  2.8762e-02, -5.9348e-03, -1.4912e-02,  2.7102e-02,\n",
       "         -1.3664e-02, -9.9282e-03, -1.3380e-02, -1.5455e-02,  1.7812e-02,\n",
       "          2.0769e-02, -2.6720e-02,  1.9830e-02,  1.3241e-02,  1.6200e-02,\n",
       "         -2.3283e-02,  1.5969e-02, -9.3796e-03, -1.9109e-02, -8.8091e-03,\n",
       "         -3.3644e-03,  2.0112e-03, -1.3490e-03, -1.2946e-02, -1.4998e-02,\n",
       "         -1.3929e-02,  3.0322e-02,  3.3482e-02, -1.5245e-02,  1.7808e-02,\n",
       "         -1.3369e-02, -3.5230e-03, -2.6577e-02,  5.2185e-03, -2.1949e-02,\n",
       "          1.7526e-02, -1.4812e-02, -2.5120e-02, -2.7304e-02, -6.4900e-03,\n",
       "          2.6931e-02, -4.8915e-03, -1.6193e-02, -1.8093e-02, -2.7480e-02,\n",
       "         -2.2426e-02, -1.6933e-02, -2.7511e-02,  2.8192e-02,  3.0066e-02,\n",
       "          2.3603e-02,  2.7548e-03,  2.3467e-02, -3.0247e-02, -3.3032e-02,\n",
       "         -3.2134e-02,  1.1493e-02, -1.4616e-02,  3.2336e-02, -6.5365e-03,\n",
       "          9.2641e-03, -2.0679e-02, -1.2835e-02, -2.9702e-02,  1.6457e-02,\n",
       "         -2.8734e-02,  7.7036e-03,  3.6636e-04,  2.5059e-02,  3.8909e-02,\n",
       "         -1.7140e-02,  1.3488e-02,  4.2397e-02, -1.4755e-02, -2.7594e-02,\n",
       "          3.4239e-02,  7.5267e-03, -5.6467e-03, -1.0751e-02,  1.5651e-02,\n",
       "         -2.7971e-02,  3.3533e-02, -1.1796e-02, -3.1209e-02, -1.8147e-03,\n",
       "         -1.7229e-02, -1.7704e-02,  3.0906e-02,  3.4601e-02,  2.3992e-02,\n",
       "         -1.0005e-02,  3.1565e-02,  1.6375e-02,  5.7394e-02,  3.3995e-02,\n",
       "          1.0130e-01,  5.5838e-02,  6.3615e-02,  9.8516e-02,  3.4183e-02,\n",
       "          7.6400e-02,  9.9627e-03,  1.0298e-02, -1.1828e-02,  1.8000e-02,\n",
       "          2.9511e-03, -4.1876e-03, -4.9810e-03,  8.9143e-03,  1.0961e-02,\n",
       "         -2.7633e-02, -1.5419e-02,  2.1506e-02, -9.5942e-03, -9.5455e-03,\n",
       "          4.1785e-02,  3.2756e-02,  3.2695e-02,  3.4433e-02,  3.9590e-02,\n",
       "          6.2741e-02,  1.0623e-01,  1.7252e-01,  1.9560e-01,  2.0492e-01,\n",
       "          1.8641e-01,  1.5915e-01,  1.1816e-01,  1.0223e-01,  6.2430e-02,\n",
       "          6.9835e-02,  4.9761e-02,  2.3450e-03,  3.5019e-02,  2.4527e-05,\n",
       "          3.4269e-02,  1.3079e-02, -1.5348e-02, -3.4424e-02, -1.1271e-03,\n",
       "          1.0604e-02,  3.4737e-02, -8.1555e-03, -1.8451e-02,  5.2695e-02,\n",
       "          5.2326e-02,  7.5997e-02,  9.1470e-02,  1.0667e-01,  1.4695e-01,\n",
       "          1.9775e-01,  1.9339e-01,  2.0475e-01,  2.2302e-01,  1.8626e-01,\n",
       "          2.0357e-01,  1.3581e-01,  7.5201e-02,  8.7935e-02,  2.7106e-02,\n",
       "         -2.6265e-03, -2.9067e-02, -3.9234e-03,  3.2884e-02,  2.1228e-02,\n",
       "          5.0149e-03,  1.1808e-02,  1.6992e-02, -4.1587e-03,  1.6574e-02,\n",
       "         -7.2382e-03,  4.1850e-02,  5.8011e-02,  2.2976e-02,  6.0592e-02,\n",
       "          8.6943e-02,  1.2480e-01,  1.0347e-01,  1.0801e-01,  1.0652e-01,\n",
       "          1.3249e-01,  1.7443e-01,  1.3202e-01,  1.8851e-01,  1.3083e-01,\n",
       "          1.0194e-01,  1.1064e-01,  3.9166e-02,  2.4958e-02, -2.5163e-02,\n",
       "         -1.8021e-02,  2.3582e-02,  2.1843e-02, -1.9590e-02,  1.9966e-02,\n",
       "         -2.0352e-02, -1.0910e-02,  3.8176e-02,  3.9783e-02,  2.9561e-02,\n",
       "          2.4151e-02,  2.3900e-02,  4.0253e-02,  7.5923e-02,  9.4050e-02,\n",
       "          1.0916e-01,  9.4191e-02,  9.4863e-02,  8.6809e-02,  7.9794e-02,\n",
       "          1.1646e-01,  1.7043e-01,  1.8015e-01,  1.2149e-01,  4.1848e-02,\n",
       "          5.8062e-02,  3.6269e-02,  1.0724e-02,  1.2565e-02,  2.6944e-02,\n",
       "         -2.6098e-02,  8.5111e-03, -2.9856e-03,  1.2700e-02,  2.7471e-02,\n",
       "         -2.5137e-02,  3.2299e-02,  1.0874e-02,  4.3332e-02,  5.5192e-02,\n",
       "          3.2591e-02,  3.9470e-02,  2.5071e-02,  4.6450e-02,  5.5923e-02,\n",
       "          5.9797e-02,  5.8540e-02,  1.1545e-01,  1.2339e-01,  1.8244e-01,\n",
       "          1.4621e-01,  1.2735e-01,  6.3370e-02,  2.0635e-02, -7.2564e-03,\n",
       "          3.6664e-02, -2.7850e-02,  2.2727e-02,  2.0073e-02,  2.4158e-02,\n",
       "         -2.4595e-02, -3.2527e-02, -1.9031e-03, -2.7484e-02,  6.5780e-03,\n",
       "          1.4892e-02, -1.9841e-02, -2.4061e-03, -1.2441e-03,  2.9048e-03,\n",
       "          1.7292e-02,  9.8531e-03,  3.9819e-02,  5.5949e-02,  1.3045e-01,\n",
       "          1.7452e-01,  1.4558e-01,  1.3033e-01,  1.5560e-01,  7.2976e-02,\n",
       "          6.6284e-02, -1.6978e-02,  3.4248e-02,  2.4516e-02, -6.3470e-05,\n",
       "         -3.4099e-02, -2.7883e-02, -5.5021e-03,  1.2745e-02,  2.3297e-02,\n",
       "         -2.6915e-02, -3.4014e-02,  4.0829e-03,  1.9128e-02,  2.0303e-02,\n",
       "          4.2772e-03,  1.6136e-02,  4.8988e-02,  4.3105e-02,  2.8445e-02,\n",
       "          6.0792e-02,  1.1222e-01,  1.9085e-01,  1.5233e-01,  1.7583e-01,\n",
       "          1.4020e-01,  7.6812e-02,  8.1084e-02,  2.3395e-02, -3.6636e-04,\n",
       "          1.4773e-02,  3.2715e-02,  1.5057e-02, -1.1234e-02,  2.0347e-03,\n",
       "         -1.9440e-02, -2.5043e-02,  1.2529e-02, -2.7581e-02, -8.9704e-03,\n",
       "         -3.4554e-02, -3.3528e-02, -1.5302e-02, -2.1189e-02, -2.0606e-02,\n",
       "          1.8906e-02,  3.4739e-02,  1.1197e-01,  1.2378e-01,  1.8660e-01,\n",
       "          1.8103e-01,  2.1068e-01,  1.8804e-01,  1.2512e-01,  6.4767e-02,\n",
       "          3.6974e-02,  7.2623e-03, -1.4574e-02,  3.4413e-02, -9.4448e-03,\n",
       "          1.2869e-02, -1.2041e-02, -3.0439e-02,  4.8697e-03, -3.0415e-02,\n",
       "         -3.0281e-02, -1.2243e-02, -4.1537e-03,  2.7928e-02,  2.2491e-02,\n",
       "         -2.8449e-02,  9.8395e-03,  1.3797e-02,  8.2459e-02,  1.2201e-01,\n",
       "          1.1036e-01,  1.5434e-01,  1.8976e-01,  2.0068e-01,  1.8702e-01,\n",
       "          1.8819e-01,  1.3931e-01,  9.2238e-02,  5.6690e-02,  4.0712e-02,\n",
       "          4.1669e-02, -6.4030e-03,  1.3098e-03,  1.4310e-02,  3.0050e-02,\n",
       "         -3.9917e-03, -3.2194e-02, -1.3984e-02, -3.0394e-02,  2.2863e-02,\n",
       "          2.9055e-02,  2.9059e-02,  1.0558e-02,  3.9968e-03, -2.1601e-02,\n",
       "          4.2983e-03,  2.2526e-02,  7.5690e-02,  1.5810e-01,  1.4205e-01,\n",
       "          1.7223e-01,  2.0020e-01,  1.8783e-01,  1.2104e-01,  1.6161e-01,\n",
       "          1.3310e-01,  6.4377e-02,  1.1747e-02,  4.8077e-02, -2.5156e-02,\n",
       "          2.6935e-03, -6.9469e-03,  8.8599e-03,  2.5156e-02, -2.3020e-02,\n",
       "         -3.4453e-02, -3.4789e-02,  2.5544e-02,  3.0262e-02,  3.8111e-02,\n",
       "          4.5897e-03, -4.7780e-03, -1.7532e-02, -1.0031e-02,  3.2234e-02,\n",
       "          6.6466e-02,  9.0828e-02,  1.0640e-01,  1.2821e-01,  1.1825e-01,\n",
       "          1.3399e-01,  1.6554e-01,  1.2506e-01,  1.2956e-01,  7.5036e-02,\n",
       "          5.0281e-02,  1.8127e-02,  3.0702e-02, -8.7943e-04,  1.7448e-02,\n",
       "          3.7610e-03, -2.5363e-02, -5.5571e-03,  2.8997e-02, -1.5349e-02,\n",
       "         -1.5336e-02, -5.5268e-03, -1.5572e-02,  2.2308e-02, -2.1819e-02,\n",
       "         -2.3215e-02,  3.2868e-02,  3.2064e-02,  2.4493e-02,  5.2573e-02,\n",
       "          4.1125e-02,  6.6156e-02,  4.8479e-02,  6.8063e-02,  1.3914e-01,\n",
       "          1.6264e-01,  1.4846e-01,  1.3842e-01,  5.7715e-02,  6.7251e-02,\n",
       "          1.7856e-02,  4.1292e-02,  2.7590e-02, -1.5721e-02,  1.0635e-02,\n",
       "         -3.0925e-02, -9.4164e-04,  2.3692e-02, -1.7478e-02, -2.1287e-02,\n",
       "          2.1992e-02,  3.1451e-02,  2.6968e-03, -8.1309e-03, -1.8984e-02,\n",
       "          8.1131e-04,  5.3537e-02,  2.4909e-02,  1.6014e-02,  1.9108e-02,\n",
       "          7.1724e-02,  7.2393e-02,  7.5330e-02,  9.0633e-02,  1.2977e-01,\n",
       "          1.4139e-01,  6.9158e-02,  2.4056e-02,  7.0159e-03, -8.6380e-03,\n",
       "         -1.9052e-02,  2.2389e-02, -1.2901e-02, -2.2824e-02,  2.7669e-02,\n",
       "          1.5049e-02, -2.6591e-02,  1.0510e-02,  1.5936e-02,  1.8439e-02,\n",
       "          1.2778e-02,  2.2903e-02,  5.0664e-02,  2.0092e-02,  2.4801e-02,\n",
       "          2.4274e-02, -6.3380e-03, -4.7594e-03,  4.1543e-02,  8.7068e-02,\n",
       "          1.1844e-01,  1.4253e-01,  1.6451e-01,  1.4941e-01,  7.9833e-02,\n",
       "          4.5906e-02,  4.7339e-04,  5.3392e-03,  4.2516e-03, -1.3175e-02,\n",
       "          2.2751e-02,  2.9385e-02, -1.9527e-02, -2.9484e-02, -2.6177e-02,\n",
       "         -8.9679e-03,  4.0598e-02,  5.9939e-02,  6.3608e-02,  6.6546e-02,\n",
       "          8.5974e-02,  5.8031e-02,  2.6746e-02,  1.5561e-02,  7.5493e-02,\n",
       "          3.7356e-02,  6.2994e-02,  8.1632e-02,  1.0794e-01,  1.5078e-01,\n",
       "          1.7027e-01,  1.4389e-01,  1.0034e-01,  4.2260e-02,  3.2939e-02,\n",
       "          3.5359e-02,  3.5867e-02,  2.8417e-02, -1.8715e-02,  4.5157e-03,\n",
       "         -2.5076e-02, -2.6670e-02,  2.2691e-02,  3.0950e-02,  3.6796e-02,\n",
       "          5.3463e-02,  4.5911e-02,  7.2188e-02,  7.3606e-02,  8.1841e-02,\n",
       "          1.0390e-01,  4.9384e-02,  4.6269e-02,  8.9402e-02,  8.1288e-02,\n",
       "          1.1757e-01,  1.7010e-01,  1.7482e-01,  1.0287e-01,  7.8383e-02,\n",
       "          6.7565e-02,  3.3527e-02,  3.6093e-02, -2.6067e-02,  1.4776e-02,\n",
       "          1.9765e-03, -2.8321e-02,  2.9211e-02, -1.4978e-02,  2.6095e-03,\n",
       "          3.2541e-02,  1.6504e-02,  5.0801e-02,  8.1432e-02,  7.8763e-02,\n",
       "          9.8376e-02,  1.4643e-01,  1.1163e-01,  1.5002e-01,  1.0800e-01,\n",
       "          1.6352e-01,  1.5895e-01,  1.3382e-01,  1.5062e-01,  1.6840e-01,\n",
       "          1.4442e-01,  9.0816e-02,  7.1946e-02,  2.5022e-02,  2.3717e-02,\n",
       "          5.5794e-03, -9.2880e-03,  4.1823e-03, -1.8770e-02, -2.0644e-02,\n",
       "          1.1819e-02,  3.3947e-02, -1.5800e-03,  1.7574e-02,  6.7226e-03,\n",
       "         -1.6319e-02,  1.2493e-02,  6.2788e-02,  1.0666e-01,  1.3902e-01,\n",
       "          1.7315e-01,  1.3853e-01,  1.7505e-01,  1.6645e-01,  1.6881e-01,\n",
       "          1.8374e-01,  1.7192e-01,  1.4053e-01,  1.0011e-01,  7.8526e-02,\n",
       "          1.9665e-02,  2.4045e-02,  3.3874e-02,  1.3487e-02, -3.5110e-02,\n",
       "          1.8767e-02, -2.8404e-02, -3.1454e-02,  1.8950e-02, -2.2238e-02,\n",
       "          2.9191e-02, -1.0645e-02, -1.7049e-02, -8.9694e-03, -9.7682e-03,\n",
       "          7.7601e-02,  8.0650e-02,  1.0791e-01,  1.4525e-01,  1.8392e-01,\n",
       "          1.4810e-01,  1.6808e-01,  1.9651e-01,  1.2284e-01,  1.3122e-01,\n",
       "          1.1029e-01,  8.5786e-02, -5.4048e-03,  1.8235e-02,  3.6027e-03,\n",
       "          3.3536e-02,  3.4642e-02,  7.9541e-03, -2.7660e-02,  3.2154e-02,\n",
       "          3.6252e-03, -9.8775e-03, -1.3837e-02, -2.9215e-02,  1.7278e-02,\n",
       "         -2.1097e-02, -1.2025e-02, -1.9128e-02,  4.5893e-02, -9.4726e-03,\n",
       "         -5.5774e-03,  3.5722e-02,  1.8797e-02,  3.7900e-02,  8.9831e-02,\n",
       "          8.6066e-02,  8.8857e-02,  4.6592e-02,  4.7309e-02,  3.1437e-03,\n",
       "          2.5976e-02, -1.3538e-03, -1.4142e-03, -3.2581e-02,  2.3263e-02,\n",
       "          3.7203e-03,  7.0348e-03, -1.5393e-02,  3.5223e-02,  3.0150e-02,\n",
       "         -2.2475e-02,  1.8295e-02, -1.7371e-02,  2.8244e-02,  2.5465e-02,\n",
       "          3.1467e-02,  2.6577e-02, -1.9142e-02, -2.5417e-02, -2.8815e-02,\n",
       "         -2.5546e-02,  3.1743e-02,  1.2256e-02, -5.9760e-04, -2.0972e-02,\n",
       "          2.0526e-03,  2.0823e-02, -2.9784e-02,  2.0474e-02, -5.7724e-03,\n",
       "         -2.6913e-02,  2.1221e-02, -2.9885e-02,  3.2782e-02,  1.6876e-02,\n",
       "         -3.4856e-02, -6.8806e-04,  4.2439e-03, -1.4447e-02, -1.5846e-02,\n",
       "          6.7939e-03, -2.3415e-02, -1.4261e-03, -9.0149e-03, -1.8134e-02,\n",
       "         -1.0842e-02, -4.3953e-03, -1.8634e-02,  5.0324e-04, -9.4853e-03,\n",
       "         -2.8645e-02, -1.1950e-02, -3.9840e-03, -9.9846e-03, -3.1844e-02,\n",
       "         -3.4706e-02,  2.8474e-02, -3.3084e-02,  1.6134e-02,  3.2863e-02,\n",
       "         -1.8635e-02, -3.5689e-02,  1.8147e-02,  3.0394e-02,  5.9568e-04,\n",
       "          6.1400e-03, -1.8673e-02, -2.0587e-02, -2.6435e-02, -4.1265e-03,\n",
       "          2.4536e-02,  8.9661e-03, -3.0523e-02,  3.3861e-02, -4.3669e-03,\n",
       "         -2.6012e-02,  2.9156e-02, -9.7278e-03, -9.7259e-04,  3.2364e-02,\n",
       "          1.1717e-02,  3.4205e-02, -3.1435e-02,  2.7367e-03,  1.5324e-02,\n",
       "         -2.9383e-02,  2.1888e-02, -3.4564e-02,  1.7522e-02, -1.0605e-02,\n",
       "         -2.5665e-02,  3.1647e-02, -2.8774e-02, -1.2756e-02]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17703c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
