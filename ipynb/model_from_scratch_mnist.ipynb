{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780a5bd3",
   "metadata": {},
   "source": [
    "# Creating a Digit Classifier (Almost) from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9548ab48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krm/mambaforge/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_cuda.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from fastai.vision.all import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1973927",
   "metadata": {},
   "source": [
    "We're creating a model that can classify any images as a 3 or a 7. We'll use a sample of MNIST that contains just these.\n",
    "\n",
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e7d01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e78f1f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('/home/krm/.fastai/data/mnist_sample/labels.csv'),Path('/home/krm/.fastai/data/mnist_sample/train'),Path('/home/krm/.fastai/data/mnist_sample/valid')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccddb63",
   "metadata": {},
   "source": [
    "The sample data is divided into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9525ec2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((#2) [Path('/home/krm/.fastai/data/mnist_sample/train/7'),Path('/home/krm/.fastai/data/mnist_sample/train/3')],\n",
       " (#2) [Path('/home/krm/.fastai/data/mnist_sample/valid/7'),Path('/home/krm/.fastai/data/mnist_sample/valid/3')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/\"train\").ls(), (path/\"valid\").ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e10436",
   "metadata": {},
   "source": [
    "Get a list of the training set of 3s and 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee0f5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = (path/\"train\"/\"3\").ls().sorted()\n",
    "sevens = (path/\"train\"/\"7\").ls().sorted()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac100836",
   "metadata": {},
   "source": [
    "Have a look at one of the 3s. We use the `Image` class from PIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d9c6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nGNgGLyAUSh2yb////79m44hxasy/e/fv3///r31/ioXumTk379/v64JCgpimvJ3GlSMBSH9f333KQYGBlVZBjV0nYLTjRkYGBgYNJ7+/TsZu6OCz739+/cyBxYZt/7Pv//+/bvVCIscyxmIc7PgIkwISUZ5BoYHqxL/CmO1MG+9jxADw88vdtjdw8DAwHD1bw0WY6FgLUMQbp3v/p5D08m897woVISb4TCacvEbf+dD1AX8/huBbtjyv38VGRgYGOy//b0ijy4psvfv7QIGhvCLyIEABzZf/v44efLj37+XPbC4MvDn379///69iD2AhE/+/XszGiMVUB8AAIBLZ4nJiClnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img3_path = threes[2]\n",
    "img3 = Image.open(img3_path)\n",
    "img3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8c273d",
   "metadata": {},
   "source": [
    "We can 'see' the image as a collection of digits using numpy arrays of pytorch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46d85d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,  13,  36],\n",
       "       [  0,   0,   0,   0,  89, 253],\n",
       "       [  0,   0,   0,   0,  89, 253],\n",
       "       [  0,   0,   0,   0,  17, 151]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array(img3)[4:10, 4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8730aae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  13,  36],\n",
       "        [  0,   0,   0,   0,  89, 253],\n",
       "        [  0,   0,   0,   0,  89, 253],\n",
       "        [  0,   0,   0,   0,  17, 151]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(img3)[4:10, 4:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8282421",
   "metadata": {},
   "source": [
    "Next we create tensors for each of the 3s and 7s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c1eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_tensors = [tensor(Image.open(path)) for path in threes]\n",
    "seven_tensors = [tensor(Image.open(path)) for path in sevens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8ebff9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(three_tensors), len(seven_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9cee3",
   "metadata": {},
   "source": [
    "Use fastai's show_image to see a seven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ccc525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(seven_tensors[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1cd89",
   "metadata": {},
   "source": [
    "We stack each list of tensors into a single one of 3 axes (rank 3), convert to float for some operations. We also scale to values between 0 and 1 (better for the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e54f65b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6131, 28, 28]), torch.Size([6265, 28, 28]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_threes = torch.stack(three_tensors).float() / 255\n",
    "stacked_sevens = torch.stack(seven_tensors).float() / 255\n",
    "stacked_threes.shape, stacked_sevens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7dfac",
   "metadata": {},
   "source": [
    "We create a training input collection by concatenating the two stacked tensors into one. We use the `view` method to reshape the tensor into two dimensions. `-1` means making the first dimension as big as possible to accomodate the new shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "439bd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36dcd0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d635368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2efe0d",
   "metadata": {},
   "source": [
    "Each item along axis 0 is now a list of 784 floats representing a single image.\n",
    "\n",
    "We next create labels for our training input. Our objective is to classify whether an image is a 3 or not. The labels for 3s will be 1 (True) and for 7s will be 0 (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df28c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_flat = tensor([1] * len(threes) + [0] * len(sevens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa67743a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad92665",
   "metadata": {},
   "source": [
    "We need to have a 2D tensor with the second dimension being a size of 1. We use pytorch's `unsqueeze` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f60d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y_flat.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d37ddca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dadefa",
   "metadata": {},
   "source": [
    "In pytorch, a dataset needs to return a tuple of (x, y) when indexed. We therefore zip training input and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "290986ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = list(zip(train_x, train_y))\n",
    "x0, y0 = dataset[0]\n",
    "x0.shape, y0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad28448",
   "metadata": {},
   "source": [
    "We do the same preparation to the validation data as we've done for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8f59e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_threes = (path/\"valid\"/\"3\").ls().sorted()\n",
    "v_sevens = (path/\"valid\"/\"7\").ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "476331ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_three_tensors = [tensor(Image.open(path)) for path in v_threes]\n",
    "v_seven_tensors = [tensor(Image.open(path)) for path in v_sevens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccded4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_v_threes = torch.stack(v_three_tensors).float()/255\n",
    "stacked_v_sevens = torch.stack(v_seven_tensors).float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e337bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = torch.cat([stacked_v_threes, stacked_v_sevens]).view(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1362ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038, 784])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d606dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2038, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_y = tensor([1] * len(v_threes) + [0] * len(v_sevens)).unsqueeze(1)\n",
    "valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0cc08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = list(zip(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d3b46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce7c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
